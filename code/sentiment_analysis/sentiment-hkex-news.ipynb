{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "dir_name= os.getcwd()+'/sentiment-data/'\n",
    "hkex_files=os.path.join(dir_name,'stock_ticker_datasets/hkex.csv')\n",
    "\n",
    "hkex=pd.read_csv(hkex_files) \n",
    "# read in the equity stock of hkex\n",
    "hkex=hkex.loc[hkex['Category'] == 'Equity']\n",
    "\n",
    "hkex['Ticker']=hkex['Ticker'].astype(str)\n",
    "hkex_input=hkex['Ticker']\n",
    "\n",
    "#chunk row size\n",
    "n = 500 \n",
    "\n",
    "print(hkex_input)\n",
    "hkex_df = [hkex_input[i:i+n] for i in range(0,len(hkex_input),n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for Vader Analysis \n",
    "\n",
    "# read from vader scores\n",
    "def read_news_vader_path(df):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    # append a compound score to every news row\n",
    "    for row in range(len(df)):\n",
    "        cs.append(analyser.polarity_scores(df['news'].iloc[row])['compound'])\n",
    "    # append the column to original dataset\n",
    "    df['compound_vader_score']=cs\n",
    "    return df\n",
    "\n",
    "\n",
    "# group by the mean compound vader score by dates\n",
    "def find_news_vader_pred_label(df,threshold):\n",
    "    print('find_pred_label')\n",
    "    news = df['news']\n",
    "    # group the data by dates\n",
    "    df = df.groupby(['dates'])['compound_vader_score'].mean().reset_index()\n",
    "    print(df)\n",
    "    final_label=[]\n",
    "    \n",
    "    # convert the vader score using a threshold to a sentiment label\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df['compound_vader_score'].iloc[i] > threshold:\n",
    "            final_label.append(2)\n",
    "        elif df['compound_vader_score'].iloc[i] < -threshold:\n",
    "            final_label.append(0)\n",
    "        elif (df['compound_vader_score'].iloc[i] >= -threshold  \n",
    "              and df['compound_vader_score'].iloc[i] <= threshold):\n",
    "            final_label.append(1)\n",
    "\n",
    "    df['vader_label'] = final_label\n",
    "    return df\n",
    "\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_vader_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "    vader_data = df\n",
    "    vader_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(vader_data,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for Textblob Analysis ###\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "# append the normalize textblob (-1 to 1) score to the corresponding news\n",
    "def read_news_textblob_path(df):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    # append a compound score to every news row\n",
    "    for row in range(len(df)):\n",
    "        cs.append(TextBlob(df['news'].iloc[row]).sentiment[0])\n",
    "    # append the column to original dataset\n",
    "    df['compound_textblob_score']=cs\n",
    "#     print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# group by the mean compound textblob score by dates\n",
    "def find_news_textblob_pred_label(df,threshold):\n",
    "    print('find_pred_label')\n",
    "#     print(df)\n",
    "    news = df['news']\n",
    "    # group the data by dates\n",
    "    df = df.groupby(['dates'])['compound_textblob_score'].mean().reset_index()\n",
    "    final_label=[]\n",
    "    \n",
    "    # convert the vader score using a threshold to a sentiment label\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df['compound_textblob_score'].iloc[i] > threshold:\n",
    "            final_label.append(2)\n",
    "        elif df['compound_textblob_score'].iloc[i] < -threshold:\n",
    "            final_label.append(0)\n",
    "        elif (df['compound_textblob_score'].iloc[i] >= -threshold  \n",
    "              and df['compound_textblob_score'].iloc[i] <= threshold):\n",
    "            final_label.append(1)\n",
    "\n",
    "    df['textblob_label'] = final_label\n",
    "    return df\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_textblob_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "    textblob_data = df\n",
    "    textblob_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(textblob_data,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "import math\n",
    "\n",
    "\n",
    "def build_dict():\n",
    "    \n",
    "    #Load master dictionary\n",
    "    path = os.path.join(dir_name,'train-data/hkex/LoughranMcDonald_MasterDictionary_2016.csv') \n",
    "    \n",
    "    master = pd.read_csv(path)\n",
    "#     print(master)\n",
    "    positive = master[master['Positive']>0]\n",
    "#     print('positive %s', positive)\n",
    "    negative = master[master['Negative']>0]\n",
    "#     print('negative %s' negative)\n",
    "    pos_words = positive['Word'].tolist()\n",
    "    neg_words = negative['Word'].tolist()\n",
    "    \n",
    "    pos_words = [word.lower() for word in pos_words]\n",
    "    neg_words = [word.lower() for word in neg_words]\n",
    "    stopwords = pd.read_csv(\"https://drive.google.com/file/d/0B4niqV00F3msSktONVhfaElXeEk/view?usp=sharing\",names=['Word'])\n",
    "    stop_list = stopwords['Word'].tolist()\n",
    "    return pos_words,neg_words,stop_list\n",
    "\n",
    "\n",
    "def process_news_Loughran(df,pos_words,neg_words,stop_lists):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    \n",
    "    # tokenize the words in each news\n",
    "    for row in range(len(df)):\n",
    "        cs.append(word.lower() for word in wpt().tokenize(df['news'].iloc[row]))\n",
    "\n",
    "#     Remove the stopwords\n",
    "    words_new = [word for word in cs if word not in stop_lists]\n",
    "\n",
    "    Loughran_label=[]\n",
    "    for words_new in cs:\n",
    "       \n",
    "        words_new_pos = [word for word in words_new if word in pos_words]\n",
    "        words_new_neg = [word for word in words_new if word in neg_words]\n",
    "        \n",
    "        if(len(words_new_pos)>len(words_new_neg)):\n",
    "            Loughran_label.append(2)\n",
    "        elif(len(words_new_pos)<len(words_new_neg)):\n",
    "            Loughran_label.append(0)\n",
    "        else:\n",
    "            Loughran_label.append(1)\n",
    "        print(Loughran_label)\n",
    "        \n",
    "    df['loughran_label']=Loughran_label\n",
    "    df = df.groupby(['dates'])['loughran_label'].mean().round().reset_index()\n",
    "#     print(df)\n",
    "    return df\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_textblob_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "\n",
    "    df.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(df,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for vader sentiment analysis ###\n",
    "\n",
    "def starter_vader(path,result_path):\n",
    "    # get the full path of each ticker\n",
    "   \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    # read append the compound vader score to the pandas dataframe\n",
    "    df = read_news_vader_path(df)\n",
    "    # pass in the threshold to get the vader label\n",
    "    df = find_news_vader_pred_label(df,0.01)\n",
    "    # store to the csv file if the dataset is not empty\n",
    "    if (df.empty == False):\n",
    "        df.to_csv(result_path,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for textblolb sentiment analysis ###\n",
    "\n",
    "def starter_textblob(path,result_path):\n",
    "          \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    # read append the compound vader score to the pandas dataframe\n",
    "    df = read_news_textblob_path(df)\n",
    "    # pass in the threshold to get the vader label\n",
    "    df = find_news_textblob_pred_label(df,0.01)\n",
    "    \n",
    "    db_df=pd.read_csv(result_path)\n",
    "\n",
    "    db_df['textblob_label']=df['textblob_label']\n",
    "    \n",
    "    # store to the csv file if the dataset is not empty\n",
    "#     print(db_df)\n",
    "    if (db_df.empty == False):\n",
    "        db_df.to_csv(result_path,index=False)\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for textblolb sentiment analysis ###\n",
    "\n",
    "def starter_Loughran(path,result_path):\n",
    "    \n",
    "    # get the full path of each ticker          \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    \n",
    "    pos_words,neg_words,stop_lists= build_dict()\n",
    "    df = process_news_Loughran(df,pos_words,neg_words,stop_lists)\n",
    "\n",
    "    db_df=pd.read_csv(result_path)\n",
    "    \n",
    "    db_df['loughran_label']=df['loughran_label']\n",
    "    \n",
    "    # store to the csv file if the dataset is not empty\n",
    "    if (db_df.empty == False):\n",
    "        db_df.to_csv(result_path,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect individual sentiment label for tickers in hkex    \n",
    "def collect_individual_sentiment(ticker):\n",
    "    try:\n",
    "        print(ticker)\n",
    "        path = os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+ticker.zfill(5)+'-aastock.csv') \n",
    "        result_path = os.path.join(dir_name,'data-results/hkex-aastock/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "        starter_vader(path,result_path)  \n",
    "        starter_textblob(path,result_path)  \n",
    "        starter_Loughran(path,result_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "                \n",
    "# for tickers in hkex_df:\n",
    "#      for ticker in tickers:\n",
    "#         collect_individual_sentiment(ticker)\n",
    "            \n",
    "            \n",
    "# collect_individual_sentiment('669')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the market sentiment\n",
    "path='hkex_agg_equity_news.csv'\n",
    "result_path='hkex_agg_equity_news_label.csv'\n",
    "print(path)\n",
    "starter_vader(path,result_path)  \n",
    "starter_textblob(path,result_path)  \n",
    "starter_Loughran(path,result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "def load_data (df,path,new_path):\n",
    "    try:\n",
    "#         new_df =pd.read_csv(path,names=['dates','compound_vader_score','vader_label','textblob_label','loughran_label'],index_col='dates',usecols=['dates', 'vader_label','textblob_label'],parse_dates=['dates'], na_values=['nan'])\n",
    "        new_df =pd.read_csv(path,names=['dates','compound_vader_score','vader_label','textblob_label','loughran_label'],index_col='dates',usecols=['dates', 'vader_label','textblob_label'],parse_dates=['dates'], na_values=['nan'])\n",
    "    \n",
    "        print(new_df)\n",
    "        df=pd.merge(df,new_df, how='left', left_index=True, right_index=True)\n",
    "        df=df.reset_index()\n",
    "        df = df.rename(columns={'index': 'dates'})\n",
    "        print(df)\n",
    "        df.to_csv(new_path,index=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "                \n",
    "                \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime, time\n",
    "\n",
    "# get the existent sentiment label for all ticker\n",
    "def sentiment_label_data_range(ticker):\n",
    "    \n",
    "    dates = pd.date_range('2017-01-01','2021-03-03',freq='B')\n",
    "    dates=dates.strftime('%Y-%m-%d')\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    path = os.path.join(dir_name,'data-results/hkex-aastock/'+'data-'+ticker.zfill(5)+'-result.csv') \n",
    "    new_path = os.path.join(dir_name,'data-results/temp_result/'+'data-'+ticker.zfill(5)+'-result.csv') \n",
    "    load_data(df,path,new_path)\n",
    "    \n",
    "# sentiment_label_data_range('6618')\n",
    "# for ticker in hkex_input:\n",
    "#     sentiment_label_data_range(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the merge data with the specific time frame\n",
    "dates = pd.date_range('2017-01-01','2021-03-03',freq='B')\n",
    "dates=dates.strftime('%Y-%m-%d')\n",
    "df = pd.DataFrame(index=dates)\n",
    "path='hkex_agg_equity_news_label.csv'\n",
    "new_path='hkex_market_equity_news_label.csv'\n",
    "load_data(df,  path,new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =pd.read_csv('hkex_agg_equity_news_label.csv',names=['dates','compound_vader_score','vader_label','compound_textblob_score','textblob_label','loughran_label'])\n",
    "# df=df.drop(['compound_textblob_score'], axis=1)\n",
    "# df.to_csv('hkex_agg_equity_news_label.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vader score as the marekt label\n",
    "# usecols=['dates', 'vader_label','textblob_label','loughran_label']\n",
    "import numpy as np \n",
    "def merge_market_individual_sentiment(ticker):\n",
    "    path = os.path.join(dir_name,'data-results/temp_result/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    market_path='hkex_market_equity_news_label.csv'\n",
    "#         df =pd.read_csv(path,names=['dates','vader_label','textblob_label','loughran_label'],index_col='dates',usecols=usecols,parse_dates=['dates'], na_values=['nan'])\n",
    "    df =pd.read_csv(path, na_values=['nan'])\n",
    "  \n",
    "\n",
    "    df_market=pd.read_csv(market_path)\n",
    "    v_label=[]\n",
    "    t_label=[]\n",
    "\n",
    "   \n",
    "    for i in range(0,len(df)):\n",
    "\n",
    "        isnull=pd.isnull(df.at[i,'vader_label'])\n",
    "        is_market_null=pd.isnull(df_market.at[i,'vader_label'])\n",
    "        \n",
    "        if(isnull==True ):\n",
    "            if(is_market_null==False):\n",
    "                v_label.append(df_market.at[i,'vader_label'])\n",
    "            else:\n",
    "                v_label.append(0)\n",
    "        else:\n",
    "            v_label.append(df.at[i,'vader_label'])\n",
    "            \n",
    "\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        isnull=pd.isnull(df.at[i,'textblob_label'])\n",
    "        is_market_null=pd.isnull(df_market.at[i,'textblob_label'])\n",
    "        if(isnull==True ):\n",
    "            if(is_market_null==False):\n",
    "                t_label.append(df_market.at[i,'textblob_label'])\n",
    "            else:\n",
    "                t_label.append(0)\n",
    "        else:\n",
    "            t_label.append(df.at[i,'textblob_label'])\n",
    "            \n",
    "            \n",
    "#     print(t_label)\n",
    "\n",
    "\n",
    "    df['vader_label']=v_label\n",
    "    df['textblob_label']=t_label\n",
    "    print(df)\n",
    "    df.to_csv(path,index=False)\n",
    "\n",
    "               \n",
    "# merge_market_individual_sentiment('669')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in hkex_input:\n",
    "    try:\n",
    "        \n",
    "        merge_market_individual_sentiment(ticker)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data for the 36 testing dataset\n",
    "test_df=['669','175','1211','2319','6186','168','2269','6618','01801','1','4','19','700','3690','9988','823','16','1113','1299','5','939','2','3','2688','941','762','6823','968','868','2382','2899','1818','2689','883','386','857']test_df=['669','175','1211','2319','6186','168','2269','6618','01801','1','4','19','700','3690','9988','823','16','1113','1299','5','939','2','3','2688','941','762','6823','968','868','2382','2899','1818','2689','883','386','857']\n",
    "\n",
    "for ticker in test_df:\n",
    "    \n",
    "    path = os.path.join(dir_name,'data-results/temp_result/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    new_path = os.path.join(dir_name,'data-results/testing/testing_df/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    df=pd.read_csv(path)\n",
    "    df=df.drop(['Unnamed: 0'],axis=1)\n",
    "    df.to_csv(new_path,index=False)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
