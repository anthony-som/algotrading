{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install BeautifulSoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir_name= os.getcwd()+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in ticker dataset from HKEX\n",
    "import pandas as pd\n",
    "hkex_files=os.path.join(dir_name,'stock_ticker_datasets/hkex.csv')\n",
    "# access the data folder to get the list of tickers in hkex\n",
    "hkex=pd.read_csv(hkex_files) \n",
    "hkex['Ticker']=hkex['Ticker'].astype(str)\n",
    "hkex_input=hkex['Ticker']\n",
    "#split datasets into smaller sets to prevent being blocked from the API\n",
    "n = 400  #chunk row size\n",
    "hkex_df = [hkex_input[i:i+n] for i in range(0,hkex_input.shape[0],n)]\n",
    "hkex.set_index(\"Ticker\" , inplace=True)\n",
    "print(hkex.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import datetime, time\n",
    "\n",
    "# prefix url to access aastock.com\n",
    "prefix_url='http://www.aastocks.com/en/stocks/analysis/stock-aafn/'\n",
    "postfix_url='/0/all/1'\n",
    "\n",
    "def get_news_aastock(ticker,days):\n",
    "\n",
    "    try:\n",
    "        # convert single digit ticker to the 5 digits to access their webpage in finviz.com\n",
    "        fill_ticker=ticker.zfill(5)\n",
    "        url=prefix_url+fill_ticker+postfix_url\n",
    "        print(url)\n",
    "        req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "        resp = urlopen(req) \n",
    "        html = BeautifulSoup(resp, features=\"lxml\")\n",
    "        # get the html code of all the corresponding dates\n",
    "        dates=html.findAll(\"div\", {\"class\": \"inline_block\"})\n",
    "\n",
    "        # get the html code of all the corresponding headlines\n",
    "        news=html.findAll(\"div\", {\"class\": \"newshead4\"})\n",
    "        idx=0\n",
    "        \n",
    "        path=os.path.join(dir_name,'data-news/data-aastock/'+'data-'+ticker+'-aastock.csv')\n",
    "        with open(path,'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for i in dates:\n",
    "                if \"/\" in str(i.get_text()):\n",
    "                    date=str(i.get_text())\n",
    "\n",
    "                    # remove the substring 'Release Time' from the date string\n",
    "                    if \"Release Time\" in date:\n",
    "                        date=date[13:23]\n",
    "                    else:\n",
    "                        date=str(date[:10])\n",
    "                        text=news[idx].get_text()\n",
    "                        # convert the date format to 'YYYY-mm-dd'\n",
    "                        date_time_obj = datetime.datetime.strptime(date, '%Y/%m/%d')\n",
    "                                                         .strftime('%Y-%m-%d')\n",
    "                        print(ticker)\n",
    "                        print(date_time_obj)\n",
    "                        if(datetime.datetime.now()-date_time_obj).days<=days:\n",
    "                            category=hkex.loc[ticker]['Category']\n",
    "                            print(text)\n",
    "                            writer.writerow([date_time_obj,text])\n",
    "                        # increment the idx to align the dates and the financial headlines\n",
    "                        idx+=1\n",
    "    except:\n",
    "        print('error')\n",
    "        pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the news for each tickers in hkex\n",
    "for tickers in hkex_df:\n",
    "    for ticker in tickers:\n",
    "        print(ticker)\n",
    "        get_news_aastock(ticker)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "df_hkex= pd.DataFrame()\n",
    "# merge all the csv files of each ticker's price movement into a single csv file\n",
    "for tickers in hkex_df:\n",
    "     for ticker in tickers:\n",
    "            try:\n",
    "                f_ticker=ticker.zfill(5)\n",
    "                l_ticker=ticker.zfill(4)\n",
    "                news_path=os.path.join(dir_name,'data-news/data-aastock/'+'data-'+f_ticker+'-aastock.csv')\n",
    "                dates_path=os.path.join(dir_name,'stock_label/hk_stock_label/'+'data-'+l_ticker+'-label.csv')\n",
    "                \n",
    "                df_news = pd.read_csv( df_news,names=['dates','news'],index_col='dates')\n",
    "                df_dates=pd.read_csv(dates_path,names=['dates','label'],index_col='dates')\n",
    "                merge=pd.merge(df_news,df_dates, how='inner', left_index=True, right_index=True)\n",
    "                df_hkex=df_hkex.append(merge)\n",
    "                print(df_hkex)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "# store as csv file in the following name\n",
    "df_hkex.to_csv('hkex_labelled_news.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime, time\n",
    "# get agg news\n",
    "df_hkex= pd.DataFrame()\n",
    "\n",
    "# merge all the csv files of each ticker's financial headlines into a single csv file\n",
    "for tickers in hkex_df:\n",
    "     for ticker in tickers:\n",
    "            try:\n",
    "#                 f_ticker=ticker.zfill(5)\n",
    "                news_path=os.path.join(dir_name,'data-news/data-aastock/'+'data-'+ticker+'-aastock.csv')\n",
    "                print(news_path)\n",
    "                df_news = pd.read_csv(news_path,names=['dates','news'])\n",
    "                print(news_path)\n",
    "                # add the category column\n",
    "                category=hkex.loc[ticker]['Category']\n",
    "                Ticker=[ticker for i in range(len(df_news))]\n",
    "                Category=[category for i in range(len(df_news))]\n",
    "                df_news['ticker']=Ticker\n",
    "                df_news['category']=Category\n",
    "                df_hkex=df_hkex.append(df_news)\n",
    "                print(df_hkex)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "# store as csv file in the following name\n",
    "hkex_path=os.path.join(dir_name,'train-data/hkex/hkex_no_labelled_news.csv')\n",
    "df_hkex.to_csv(hkex_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}