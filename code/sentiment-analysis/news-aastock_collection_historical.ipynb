{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# declare a directory name \n",
    "dir_name= os.getcwd()+'/sentiment-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "hkex_files=os.path.join(dir_name,'stock_ticker_datasets/hkex.csv')\n",
    "\n",
    "hkex=pd.read_csv(hkex_files) \n",
    "# hkex[hkex.iloc['Category'] == 'Equity Securities']\n",
    "\n",
    "hkex=hkex.loc[hkex['Category'] == 'Equity']\n",
    "\n",
    "hkex['Ticker']=hkex['Ticker'].astype(str)\n",
    "hkex_input=hkex['Ticker']\n",
    "\n",
    "n = 400  #chunk row size\n",
    "hkex_df = [hkex_input[i:i+n] for i in range(0,len(hkex_input),n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all stocks using selenium \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import datetime, time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument('no-sandbox')  \n",
    "chrome_options.add_argument('-disable-dev-shm-usage')  \n",
    "\n",
    "# target_time='2020-01-01'\n",
    "# target_time=datetime.datetime.strptime(target_time, '%Y-%m-%d')\n",
    "\n",
    "# get news from aastock.com\n",
    "def get_news_aastock(ticker,postfix_url,newstype):\n",
    "#     driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)\n",
    "    driver.implicitly_wait(20)\n",
    "#     WebDriverWait wait = new WebDriverWait(driver, 10);\n",
    "    prefix_url='http://www.aastocks.com/en/stocks/analysis/stock-aafn/'\n",
    "#     postfix_url='/0/research-report/1'\n",
    "    try:\n",
    "        SCROLL_PAUSE_TIME = 2\n",
    "        fill_ticker=ticker.zfill(5)\n",
    "        url=prefix_url+fill_ticker+postfix_url\n",
    "        driver.get(url)\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "     \n",
    "        html=BeautifulSoup(driver.page_source, 'lxml')\n",
    "      \n",
    "        dates=html.findAll(\"div\", {\"class\": \"newstime4\"})\n",
    "        \n",
    "        news=html.findAll(\"div\", {\"class\": \"newshead4\"})\n",
    "        \n",
    "        idx=0\n",
    "        path=os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+fill_ticker+'-aastock.csv')\n",
    "        if (len(dates)>0):\n",
    "            with open(path,'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                for i in dates:\n",
    "                    \n",
    "                    if \"/\" in str(i.get_text()):\n",
    "                        print(idx)\n",
    "                        \n",
    "                        date=str(i.get_text())\n",
    "                        if \"Release Time\" in date:\n",
    "                            date=date[13:23]\n",
    "                        elif (date[0]==\" \"):\n",
    "                            date=str(date[1:11])\n",
    "                        else:\n",
    "                            date=str(date[0:10])\n",
    "#                         print(date)\n",
    "                        text=news[idx].get_text()\n",
    "\n",
    "                        date_time_obj = datetime.datetime.strptime(date, '%Y/%m/%d')\n",
    "#                         if(date_time_obj>=target_time):\n",
    "                        date_time=date_time_obj.strftime('%Y-%m-%d')\n",
    "#                             print([date_time,text])\n",
    "#                         print(([date_time,text,ticker,'news']))\n",
    "                        writer.writerow([date_time,text,ticker,newstype])\n",
    "\n",
    "                        idx+=1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all stocks using selenium within a specific day\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "import datetime, time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument('no-sandbox')  \n",
    "chrome_options.add_argument('-disable-dev-shm-usage')  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# target_time='2020-01-01'\n",
    "# target_time=datetime.datetime.strptime(target_time, '%Y-%m-%d')\n",
    "\n",
    "# get news from aastock.com\n",
    "def get_news_aastock_time(ticker,postfix_url,newstype,days):\n",
    "#     driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)\n",
    "    driver.implicitly_wait(20)\n",
    "#     WebDriverWait wait = new WebDriverWait(driver, 10);\n",
    "    prefix_url='http://www.aastocks.com/en/stocks/analysis/stock-aafn/'\n",
    "#     postfix_url='/0/research-report/1'\n",
    "    try:\n",
    "        SCROLL_PAUSE_TIME = 2\n",
    "        fill_ticker=ticker.zfill(5)\n",
    "        url=prefix_url+fill_ticker+postfix_url\n",
    "        driver.get(url)\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "     \n",
    "        html=BeautifulSoup(driver.page_source, 'lxml')\n",
    "      \n",
    "        dates=html.findAll(\"div\", {\"class\": \"newstime4\"})\n",
    "        \n",
    "        news=html.findAll(\"div\", {\"class\": \"newshead4\"})\n",
    "        \n",
    "        idx=0\n",
    "        path=os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+fill_ticker+'-aastock.csv')\n",
    "        if (len(dates)>0):\n",
    "            with open(path,'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                for i in dates:\n",
    "                    \n",
    "                    if \"/\" in str(i.get_text()):\n",
    "                        print(idx)\n",
    "                        \n",
    "                        date=str(i.get_text())\n",
    "                        if \"Release Time\" in date:\n",
    "                            date=date[13:23]\n",
    "                        elif (date[0]==\" \"):\n",
    "                            date=str(date[1:11])\n",
    "                        else:\n",
    "                            date=str(date[0:10])\n",
    "#                         print(date)\n",
    "                        text=news[idx].get_text()\n",
    "\n",
    "                        date_time_obj = datetime.datetime.strptime(date, '%Y/%m/%d')\n",
    "                        if(datetime.datetime.now()-date_time_obj).days<=days:\n",
    "                            date_time=date_time_obj.strftime('%Y-%m-%d')\n",
    "                            print([date_time,text])\n",
    "                            print(([date_time,text,ticker,'news']))\n",
    "                            writer.writerow([date_time,text,ticker,newstype])\n",
    "\n",
    "                        idx+=1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the news in aastock for each ticker\n",
    "def collect_news (ticker):\n",
    "    \n",
    "    news_report_postfix_url='/0/research-report'\n",
    "    news_result_postfix_url='/0/result-announcement'\n",
    "    news_daily_postfix_url='/0/hk-stock-news'\n",
    "    news_indus_postfix_url='/0/industry-news'\n",
    "    \n",
    "    get_news_aastock(ticker,news_daily_postfix_url,'news-daily')\n",
    "    get_news_aastock(ticker,news_report_postfix_url,'news-report')\n",
    "    get_news_aastock(ticker,news_result_postfix_url,'news-result')\n",
    "    get_news_aastock(ticker,news_indus_postfix_url,'news-indus')\n",
    "\n",
    "for tickers in hkex_df:\n",
    "     for ticker in tickers:\n",
    "                try:\n",
    "                        print (ticker)\n",
    "                        collect_news(ticker)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the news in aastock for each ticker within a speicific time\n",
    "def collect_news_time (ticker,days):\n",
    "    \n",
    "    news_report_postfix_url='/0/research-report'\n",
    "    news_result_postfix_url='/0/result-announcement'\n",
    "    news_daily_postfix_url='/0/hk-stock-news'\n",
    "    news_indus_postfix_url='/0/industry-news'\n",
    "    \n",
    "    get_news_aastock_time(ticker,news_daily_postfix_url,'news-daily',days)\n",
    "    get_news_aastock_time(ticker,news_report_postfix_url,'news-report',days)\n",
    "    get_news_aastock_time(ticker,news_result_postfix_url,'news-result',days)\n",
    "    get_news_aastock_time(ticker,news_indus_postfix_url,'news-indus',days)\n",
    "\n",
    "for tickers in hkex_df:\n",
    "     for ticker in tickers:\n",
    "                try:\n",
    "                        print (ticker)\n",
    "                        collect_news_time(ticker,45)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime, time\n",
    "import pandas as pd\n",
    "\n",
    "#agg function for all equities in hkex\n",
    "def agg_news(hkex_df,df_hkex):\n",
    "    \n",
    "    for ticker in df_hkex:\n",
    "        try:\n",
    "            print(ticker)\n",
    "            news_path=os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+ticker.zfill(5)+'-aastock.csv')\n",
    "            \n",
    "            df = pd.read_csv(news_path,names=['dates','news','ticker','newstype'])\n",
    "\n",
    "            hkex_df=hkex_df.append(df)  \n",
    "            print(hkex_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n",
    "    return hkex_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# get agg news for all equities in hkex\n",
    "df_hkex= pd.DataFrame()\n",
    "df=agg_news(df_hkex,hkex_input)\n",
    "print(df)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}