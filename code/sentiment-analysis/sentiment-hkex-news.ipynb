{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "dir_name= os.getcwd()+'/sentiment-data/'\n",
    "\n",
    "\n",
    "hkex_files=os.path.join(dir_name,'stock_ticker_datasets/hkex.csv')\n",
    "\n",
    "hkex=pd.read_csv(hkex_files) \n",
    "# hkex[hkex.iloc['Category'] == 'Equity Securities']\n",
    "\n",
    "hkex=hkex.loc[hkex['Category'] == 'Equity']\n",
    "\n",
    "hkex['Ticker']=hkex['Ticker'].astype(str)\n",
    "hkex_input=hkex['Ticker']\n",
    "\n",
    "\n",
    "n = 500  #chunk row size\n",
    "# 2554\n",
    "hkex_df = [hkex_input[i:i+n] for i in range(0,len(hkex_input),n)]\n",
    "# test_df=['669','175','1211','2319','6186','168','2269','6618','01801','1','4','19','700','3690','9988','823','16','1113','1299','5','939','2','3','2688','941','762','6823','968','868','2382','2899','1818','2689','883','386','857']\n",
    "# print(len(test_df))\n",
    "# hkex.set_index(\"Ticker\" , inplace=True)\n",
    "print(hkex_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "nltk.downloader.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for Vader Analysis \n",
    "\n",
    "# read from vader scores\n",
    "def read_news_vader_path(df):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    # append a compound score to every news row\n",
    "    for row in range(len(df)):\n",
    "        cs.append(analyser.polarity_scores(df['news'].iloc[row])['compound'])\n",
    "    # append the column to original dataset\n",
    "    df['compound_vader_score']=cs\n",
    "    return df\n",
    "\n",
    "\n",
    "# group by the mean compound vader score by dates\n",
    "def find_news_vader_pred_label(df,threshold):\n",
    "    print('find_pred_label')\n",
    "    news = df['news']\n",
    "    # group the data by dates\n",
    "    df = df.groupby(['dates'])['compound_vader_score'].mean().reset_index()\n",
    "    final_label=[]\n",
    "    \n",
    "    # convert the vader score using a threshold to a sentiment label\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df['compound_vader_score'].iloc[i] > threshold:\n",
    "            final_label.append(2)\n",
    "        elif df['compound_vader_score'].iloc[i] < -threshold:\n",
    "            final_label.append(0)\n",
    "        elif (df['compound_vader_score'].iloc[i] >= -threshold  \n",
    "              and df['compound_vader_score'].iloc[i] <= threshold):\n",
    "            final_label.append(1)\n",
    "\n",
    "    df['vader_label'] = final_label\n",
    "    return df\n",
    "\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_vader_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "    vader_data = df\n",
    "    vader_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(vader_data,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for Textblob Analysis \n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "# append the normalize textblob (-1 to 1) score to the corresponding news\n",
    "def read_news_textblob_path(df):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    # append a compound score to every news row\n",
    "    for row in range(len(df)):\n",
    "        cs.append(TextBlob(df['news'].iloc[row]).sentiment[0])\n",
    "    # append the column to original dataset\n",
    "    df['compound_textblob_score']=cs\n",
    "#     print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# group by the mean compound textblob score by dates\n",
    "def find_news_textblob_pred_label(df,threshold):\n",
    "    print('find_pred_label')\n",
    "#     print(df)\n",
    "    news = df['news']\n",
    "    # group the data by dates\n",
    "    df = df.groupby(['dates'])['compound_textblob_score'].mean().reset_index()\n",
    "    final_label=[]\n",
    "    \n",
    "    # convert the vader score using a threshold to a sentiment label\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        if df['compound_textblob_score'].iloc[i] > threshold:\n",
    "            final_label.append(2)\n",
    "        elif df['compound_textblob_score'].iloc[i] < -threshold:\n",
    "            final_label.append(0)\n",
    "        elif (df['compound_textblob_score'].iloc[i] >= -threshold  \n",
    "              and df['compound_textblob_score'].iloc[i] <= threshold):\n",
    "            final_label.append(1)\n",
    "\n",
    "    df['textblob_label'] = final_label\n",
    "    return df\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_textblob_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "    textblob_data = df\n",
    "    textblob_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(textblob_data,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "import math\n",
    "\n",
    "\n",
    "def build_dict():\n",
    "    \n",
    "    #Load master dictionary\n",
    "    path = os.path.join(dir_name,'train-data/hkex/LoughranMcDonald_MasterDictionary_2016.csv') \n",
    "    \n",
    "    master = pd.read_csv(path)\n",
    "#     print(master)\n",
    "    positive = master[master['Positive']>0]\n",
    "#     print('positive %s', positive)\n",
    "    negative = master[master['Negative']>0]\n",
    "#     print('negative %s' negative)\n",
    "    pos_words = positive['Word'].tolist()\n",
    "    neg_words = negative['Word'].tolist()\n",
    "    \n",
    "    pos_words = [word.lower() for word in pos_words]\n",
    "    neg_words = [word.lower() for word in neg_words]\n",
    "    stopwords = pd.read_csv(\"https://drive.google.com/file/d/0B4niqV00F3msSktONVhfaElXeEk/view?usp=sharing\",names=['Word'])\n",
    "    stop_list = stopwords['Word'].tolist()\n",
    "    return pos_words,neg_words,stop_list\n",
    "\n",
    "\n",
    "def process_news_Loughran(df,pos_words,neg_words,stop_lists):\n",
    "    print('read in datasets')\n",
    "    cs=[]\n",
    "    \n",
    "    # tokenize the words in each news\n",
    "    for row in range(len(df)):\n",
    "        cs.append(word.lower() for word in wpt().tokenize(df['news'].iloc[row]))\n",
    "\n",
    "#     Remove the stopwords\n",
    "    words_new = [word for word in cs if word not in stop_lists]\n",
    "\n",
    "    Loughran_label=[]\n",
    "    for words_new in cs:\n",
    "       \n",
    "        words_new_pos = [word for word in words_new if word in pos_words]\n",
    "        words_new_neg = [word for word in words_new if word in neg_words]\n",
    "        \n",
    "        if(len(words_new_pos)>len(words_new_neg)):\n",
    "            Loughran_label.append(2)\n",
    "        elif(len(words_new_pos)<len(words_new_neg)):\n",
    "            Loughran_label.append(0)\n",
    "        else:\n",
    "            Loughran_label.append(1)\n",
    "        print(Loughran_label)\n",
    "        \n",
    "    df['loughran_label']=Loughran_label\n",
    "    df = df.groupby(['dates'])['loughran_label'].mean().round().reset_index()\n",
    "#     print(df)\n",
    "    return df\n",
    "\n",
    "# merge the dataset with the hang seng index daily moving average\n",
    "def merge_textblob_actual_label (df,hsi_movement_df):\n",
    "    print('merge_actual_label')\n",
    "\n",
    "    df.set_index(keys = [\"dates\"],inplace=True)\n",
    "    label_data = pd.read_csv(hsi_movement_df)\n",
    "    label_data.set_index(keys = [\"dates\"],inplace=True)\n",
    "    # inner join the two datasets using the date index\n",
    "    merge = pd.merge(df,label_data, how='inner', left_index=True, right_index=True)\n",
    "    merge = merge.reset_index()\n",
    "    # drop the redudant column \n",
    "    merge = merge.drop(['Unnamed: 0'],axis=1)\n",
    "    \n",
    "    return merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for vader sentiment analysis ###\n",
    "\n",
    "def starter_vader(ticker):\n",
    "    # get the full path of each ticker\n",
    "    path = os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+ticker.zfill(5)+'-aastock.csv')           \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    # read append the compound vader score to the pandas dataframe\n",
    "    df = read_news_vader_path(df)\n",
    "    # pass in the threshold to get the vader label\n",
    "    df = find_news_vader_pred_label(df,0.01)\n",
    "            \n",
    "    result_path = os.path.join(dir_name,'data-results/hkex-aastock/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    \n",
    "    # get the full path of the hang seng index average csv file\n",
    "    hsi_movement_path = os.path.join(dir_name,'train-data/hkex/hsi_movement.csv')  \n",
    "    # merge the df pandas with the hsi_average\n",
    "    df = merge_vader_actual_label (df,hsi_movement_path)\n",
    "    print(df)\n",
    "    # store to the csv file if the dataset is not empty\n",
    "    if (df.empty == False):\n",
    "        df.to_csv(result_path,index=False)\n",
    "\n",
    "# starter_vader('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for textblolb sentiment analysis ###\n",
    "\n",
    "def starter_textblob(ticker):\n",
    "    # get the full path of each ticker\n",
    "    path = os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+ticker.zfill(5)+'-aastock.csv')           \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    # read append the compound vader score to the pandas dataframe\n",
    "    df = read_news_textblob_path(df)\n",
    "    # pass in the threshold to get the vader label\n",
    "    df = find_news_textblob_pred_label(df,0.01)\n",
    "            \n",
    "    result_path = os.path.join(dir_name,'data-results/hkex-aastock/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    \n",
    "    # get the full path of the hang seng index average csv file\n",
    "    hsi_movement_path = os.path.join(dir_name,'train-data/hkex/hsi_movement.csv') \n",
    "    db_df=pd.read_csv(result_path)\n",
    "    # merge the df pandas with the hsi_average\n",
    "    df = merge_textblob_actual_label (df,hsi_movement_path)\n",
    "#     print(df)\n",
    "#     print(db_df)\n",
    "    db_df['compound_textblob_score']=df['compound_textblob_score']\n",
    "    db_df['textblob_label']=df['textblob_label']\n",
    "    \n",
    "    # store to the csv file if the dataset is not empty\n",
    "    print(db_df)\n",
    "    if (db_df.empty == False):\n",
    "        db_df.to_csv(result_path,index=False)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter function for Loughran dictionary sentiment analysis ###\n",
    "\n",
    "def starter_Loughran(ticker):\n",
    "    # get the full path of each ticker\n",
    "    path = os.path.join(dir_name,'data-news/data-aastock-equities/'+'data-'+ticker.zfill(5)+'-aastock.csv')           \n",
    "    df = pd.read_csv(path,names=['dates','news','ticker','newstype'])\n",
    "    # \n",
    "   \n",
    "    pos_words,neg_words,stop_lists= build_dict()\n",
    "    df = process_news_Loughran(df,pos_words,neg_words,stop_lists)\n",
    "    \n",
    "    result_path = os.path.join(dir_name,'data-results/hkex-aastock/'+'data-'+ticker.zfill(5)+'-result.csv')\n",
    "    \n",
    "    # get the full path of the hang seng index average csv file\n",
    "    hsi_movement_path = os.path.join(dir_name,'train-data/hkex/hsi_movement.csv') \n",
    "    db_df=pd.read_csv(result_path)\n",
    "    \n",
    "    df = merge_textblob_actual_label (df,hsi_movement_path)\n",
    "    db_df['loughran_label']=df['loughran_label']\n",
    "\n",
    "    \n",
    "#     print(db_df)\n",
    "    \n",
    "    # store to the csv file if the dataset is not empty\n",
    "    if (db_df.empty == False):\n",
    "        db_df.to_csv(result_path,index=False)\n",
    "# starter_Loughran('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter_vader('1') \n",
    "# starter_textblob('1')\n",
    "\n",
    "# collect sentiment label for tickers in hkex    \n",
    "for tickers in hkex_df:\n",
    "     for ticker in tickers:\n",
    "            try:\n",
    "                print(ticker)\n",
    "#                 starter_vader(ticker)  \n",
    "#                 starter_textblob(ticker)  \n",
    "                starter_Loughran(ticker)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "                \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}